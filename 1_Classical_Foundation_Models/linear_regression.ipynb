{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e412117",
   "metadata": {},
   "source": [
    "> **NOTE** Linear regression **updates weights and bias using Gradient Descent**, which utilizes the gradients of the loss function with respect to parameters to iteratively adjust parameters, reducing prediction error (loss).  \n",
    "> \n",
    "> Assume your linear regression model is:  \n",
    "> $$\n",
    "> \\hat{y} = X W + b\n",
    "> $$\n",
    "> * $X$ is the input feature matrix ($N \\times D_{in}$)\n",
    "> * $W$ is the weight matrix ($D_{in} \\times D_{out}$)\n",
    "> * $b$ is the bias vector ($D_{out}$)\n",
    "> * $\\hat{y}$ is the predicted output  \n",
    ">  \n",
    "> The loss function uses Mean Squared Error (MSE):\n",
    "> $$\n",
    "> L = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n",
    "> $$  \n",
    "> ---\n",
    "> **Update Formula (Gradient Descent)**\n",
    "> 1. Calculate weight gradient:\n",
    "> $$\n",
    "> \\frac{\\partial L}{\\partial W} = \\frac{2}{N} X^\\top (XW + b - Y)\n",
    "> $$\n",
    "> 2. Calculate bias gradient:\n",
    "> $$\n",
    "> \\frac{\\partial L}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\n",
    "> $$\n",
    "> 3. Update parameters using learning rate $\\eta$:\n",
    "> $$\n",
    "> W := W - \\eta \\frac{\\partial L}{\\partial W}\n",
    "> $$\n",
    "> $$\n",
    "> b := b - \\eta \\frac{\\partial L}{\\partial b}\n",
    "> $$\n",
    "> ---\n",
    "> **Implementation Principle in PyTorch**\n",
    "> * **Forward pass**: Compute prediction $\\hat{y}$\n",
    "> * **Calculate loss**: MSE loss\n",
    "> * **Backward pass**: Call `loss.backward()` to automatically compute gradients of $W$ and $b$ (i.e., the partial derivatives above)\n",
    "> * **Parameter update**: Update parameters based on gradients using an optimizer (e.g., SGD). MUST USE `with torch.no_grad()`\n",
    "> ---\n",
    "> \n",
    "> **Summary**\n",
    "> * Linear regression weight and bias updates are essentially gradient descent: fine-tuning parameters along the negative gradient direction.\n",
    "> * The goal is to make model predictions closer to true labels, reducing loss.\n",
    "> * PyTorch automatically handles differentiation; you just need to call `.backward()` and manually (or > automatically) update parameters.\n",
    "> * During **forward pass and loss computation**, we need gradients ‚Üí **autograd on**.\n",
    "> * During **parameter updates** (like SGD step), we don‚Äôt want to track those operations ‚Üí **autograd off**.\n",
    "> * ‚úÖ Always **zero out** gradients before calling `.backward()`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f508952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LinearRegression():\n",
    "    def __init__(self, input_dim, output_dim, learning_rate=0.01):\n",
    "        # Initialize W and B, set requires_grad=True for autograd\n",
    "        self.weights = torch.randn(input_dim, output_dim, requires_grad=True)\n",
    "        self.bias = torch.zeros(output_dim, requires_grad=True)\n",
    "        self.lr = learning_rate\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X @ self.weights + self.bias  # Be careful with the order of matrix multiplication @\n",
    "    \n",
    "    def mse_loss(self, predictions, targets):\n",
    "        return (predictions - targets).pow(2).mean()  # Mean Squared Error loss\n",
    "    \n",
    "    def step(self):\n",
    "        # SGD step to update weights and bias\n",
    "        with torch.no_grad():\n",
    "            self.weights -= self.lr * self.weights.grad\n",
    "            self.bias -= self.lr * self.bias.grad\n",
    "\n",
    "            # Zero the gradients after updating\n",
    "            self.weights.grad.zero_()\n",
    "            self.bias.grad.zero_()\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass to get predictions\n",
    "            y_pred = self.forward(X)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.mse_loss(y_pred, y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.step()\n",
    "\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eef850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 19.7387\n",
      "Epoch 10/100, Loss: 0.2763\n",
      "Epoch 20/100, Loss: 0.0107\n",
      "Epoch 30/100, Loss: 0.0084\n",
      "Epoch 40/100, Loss: 0.0084\n",
      "Epoch 50/100, Loss: 0.0084\n",
      "Epoch 60/100, Loss: 0.0084\n",
      "Epoch 70/100, Loss: 0.0084\n",
      "Epoch 80/100, Loss: 0.0084\n",
      "Epoch 90/100, Loss: 0.0084\n",
      "Epoch 100/100, Loss: 0.0084\n",
      "Prediction for x=4.0: 10.9684\n"
     ]
    }
   ],
   "source": [
    "# Sythetic dataÔºö y = 2 * x + 3\n",
    "torch.manual_seed(0)\n",
    "x_train = torch.randn(100, 1)\n",
    "y_train = 2 * x_train + 3 + 0.1 * torch.randn(100, 1)  # Adding some noise\n",
    "\n",
    "model = LinearRegression(input_dim=1, output_dim=1, lr=0.1)\n",
    "model.train(x_train, y_train, epochs=100)\n",
    "\n",
    "# test the model\n",
    "x_test = torch.tensor([[4.0]])\n",
    "y_pred = model.predict(x_test)\n",
    "print(f\"Prediction for x=4.0: {y_pred.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5773fc",
   "metadata": {},
   "source": [
    "> **TIP** Or you can use `LinearRegression()` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eddf14ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights: [ 1.99579429 -3.00505965  1.00414443]\n",
      "Learned bias: 0.48179536115176136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Generate dummy data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(100, 3)\n",
    "true_w = np.array([2.0, -3.0, 1.0])\n",
    "true_b = 0.5\n",
    "y = X @ true_w + true_b + 0.1 * np.random.randn(100)\n",
    "\n",
    "# Define and fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Parameters\n",
    "print(\"Learned weights:\", model.coef_)\n",
    "print(\"Learned bias:\", model.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65ff49",
   "metadata": {},
   "source": [
    "### RegularizationÔºöL1 vs L2\n",
    "\n",
    "Use regularization to limit the complexity of model, and prevent overfitting.\n",
    "\n",
    "#### L1 (Lasso)\n",
    "\n",
    "**Penalty termÔºö**\n",
    "\n",
    "$$\n",
    "\\| \\boldsymbol{w} \\|_1 = \\sum_{i=1}^{n} |w_i|\n",
    "$$\n",
    "\n",
    "**Function for linear regressionÔºö**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Lasso}} = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\boldsymbol{w}^\\top \\boldsymbol{x}_i)^2 + \\lambda \\sum_{j=1}^{p} |w_j|\n",
    "$$\n",
    "\n",
    "\n",
    "#### L2 (Ridge)\n",
    "\n",
    "**Penalty termÔºö**\n",
    "\n",
    "$$\n",
    "\\| \\boldsymbol{w} \\|_2^2 = \\sum_{i=1}^{n} w_i^2\n",
    "$$\n",
    "\n",
    "**Function for linear regressionÔºö**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{Ridge}} = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - \\boldsymbol{w}^\\top \\boldsymbol{x}_i)^2 + \\lambda \\sum_{j=1}^{p} w_j^2\n",
    "$$\n",
    "\n",
    "\n",
    "#### üîç Comparision\n",
    "| Property                | L1 (Lasso)                             | L2 (Ridge)                            |\n",
    "|-------------------------|----------------------------------------|----------------------------------------|\n",
    "| Objective               | Feature selection (sparsity)           | Feature shrinkage (stability)          |\n",
    "| Sparse solution         | ‚úÖ (some weights become 0)             | ‚ùå (weights are usually non-zero)      |\n",
    "| Handling correlated features | May randomly keep some features       | Compresses all features evenly         |\n",
    "| Use cases               | High-dimensional sparse data, interpretability | Multicollinearity, generalization     |\n",
    "| Convex optimization     | ‚úÖ                                      | ‚úÖ                                      |\n",
    "| Sensitivity to outliers | High (non-smooth gradient)             | Low (smooth penalty term)              |\n",
    "\n",
    "---\n",
    "\n",
    "#### Use `scikit-learn`\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "lasso = Lasso(alpha=0.1)  # L1\n",
    "ridge = Ridge(alpha=1.0)  # L2\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb6667",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zero-to-Hero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
